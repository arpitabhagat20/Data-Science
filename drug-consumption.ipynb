{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**ASSIGNMENT -1 DRUG CONSUMPTION**","metadata":{"id":"FVP6Cf9Nkk5Q","editable":false}},{"cell_type":"code","source":"#installing dependencies\n!pip install eli5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yyqR88NLzGrb","outputId":"fa59d29d-cc21-43b7-f151-ef032d8b74c2","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Attributes for Quantified Data:\n\nID: is a number of records in an original database. Cannot be related to the participant. It can be used for reference only.\n\nAge (Real) is the age of participant\n\nGender: Male or Female\n\nEducation: level of education of participant\n\nCountry: country of origin of the participant\n\nEthnicity: ethnicity of participant\n\nNscore (Real) is NEO-FFI-R Neuroticism\n\nEscore (Real) is NEO-FFI-R Extraversion\n\nOscore (Real) is NEO-FFI-R Openness to experience.\n\nAscore (Real) is NEO-FFI-R Agreeableness.\n\nCscore (Real) is NEO-FFI-R Conscientiousness.\n\nImpulsive (Real) is impulsiveness measured by BIS-11\n\nSS (Real) is sensation seeing measured by ImpSS\n\nAlcohol: alcohol consumption\n\nAmphet: amphetamines consumption\n\nAmyl: nitrite consumption\n\nBenzos: benzodiazepine consumption\n\nCaff: caffeine consumption\n\nCannabis: marijuana consumption\n\nChoc: chocolate consumption\n\nCoke: cocaine consumption\n\nCrack: crack cocaine consumption\n\nEcstasy: ecstasy consumption\n\nHeroin: heroin consumption\n\nKetamine: ketamine consumption\n\nLegalh: legal highs consumption\n\nLSD: LSD consumption\n\nMeth: methadone consumption\n\nMushroom: magic mushroom consumption\n\nNicotine: nicotine consumption\n\nSemer: class of fictitious drug Semeron consumption (i.e. control)\n\nVSA: class of volatile substance abuse consumption","metadata":{"id":"sPEpzLw-Axhr","editable":false}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot","metadata":{"id":"pREDhMq1zToT","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the ablone dataset\ndata = pd.read_csv(\"https://www.kaggle.com/datasets/obeykhadija/drug-consumptions-uci\")","metadata":{"id":"4DM3bewpkZnS","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Viewing data in the file \ndata.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"LuNY34YhzkE-","outputId":"97e82762-9474-4dbb-e808-1668dcf74fa9","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The read_csv() is a function in pandas library that is used to read a CSV file ","metadata":{"id":"zQizmd6cclyz","editable":false}},{"cell_type":"code","source":"#function for coverting binary to yes/no\n\ndef convert_binary_to_yesno(x):\n    if x == 1:\n        return \"Yes\"\n    else:\n        return \"No\"","metadata":{"id":"lVUPH9A1f0KT","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Converting ID as to a Yes/No field  for our Analysis\ndata['ID'] = data['ID'].apply(convert_binary_to_yesno)","metadata":{"id":"NrP3TJRIf1nI","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking if ID has 1 and 0 only(binary)\ndata['ID'].value_counts()\nplt.figure(figsize=(10,5))\nplt.barh(['No','Yes'],data['ID'].value_counts(), color =\"#cc7000\")\nplt.title('Distribution for ID', size =14)\nplt.xlabel(\"Count\", size =14)\nplt.ylabel(\"ID\", size =14)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":372},"id":"ZpL_8QnigU4V","outputId":"1d0a61db-82ac-46d0-a0af-da9550bb3b95","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: We hereby declare it as a yes/no field.","metadata":{"id":"0i6hoCzWobzi","editable":false}},{"cell_type":"code","source":"#Checking for duplicate values\ndata=data.drop_duplicates()\n","metadata":{"id":"VbDN_X3SvIUK","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.duplicated().value_counts())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG6E9_oxvYvC","outputId":"f49ba3c1-2e04-4d15-a206-b3090f981d2d","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What are the data types?**","metadata":{"id":"NGMPlYgCistq","editable":false}},{"cell_type":"code","source":"# checking the data types\ndata.dtypes\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwDMR600P_Ih","outputId":"7ffa8999-f385-4bc5-8f73-314fc896c106","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting the list of categorical columns\ncategorical_cols = data.select_dtypes(include=['object']).columns.tolist()","metadata":{"id":"DGwWCuY0BPq-","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Getting the list of numerical columns\nnumerical_cols = data.select_dtypes(exclude=['object']).columns.tolist()","metadata":{"id":"r38wmPlUBS9g","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing the list of categorical and numerical columns\nprint(\"--------------------------------------------------------\")\nprint(\"                 Categorical Variables                  \")\nprint(\"--------------------------------------------------------\")\nprint(f'Total number of categorical variables in our dataset: {len(categorical_cols)}')\nfor row,col in enumerate(categorical_cols):\n    print(f'{row+1}. {col}')\nprint(\"\\n\")\nprint(\"--------------------------------------------------------\")\nprint(\"                 Numerical Variables                  \")\nprint(\"--------------------------------------------------------\")\nprint(f'Total number of numerical variables in our dataset: {len(numerical_cols)}')\nfor row,col in enumerate(numerical_cols):\n    print(f'{row+1}. {col}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsTOfk1jBXKG","outputId":"e81da751-80c6-4184-a824-e00677994a8e","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence,  we seperated the caterogical and numerical data.","metadata":{"id":"ZmiGqdeV0TUy","editable":false}},{"cell_type":"markdown","source":"**Are there any missing values?**","metadata":{"id":"8JiECRWFc9_m","editable":false}},{"cell_type":"code","source":"#checking if the any data is missing\ndata.isnull().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R82oQkXyBl7Z","outputId":"740b61f3-3e36-4eb1-f90a-372ad5ff2797","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have no missing values in our independent and dependent variables.","metadata":{"id":"imTGs2B5pEs_","editable":false}},{"cell_type":"markdown","source":"**Checking for naN**","metadata":{"id":"OFKIYT1odCct","editable":false}},{"cell_type":"code","source":"data.isna().sum()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOQL-6cYBtDe","outputId":"fd345492-9928-48bf-9b7e-2bb9d4220160","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence no missing values in dependent and independent variables","metadata":{"id":"BMp9Oon_dK_n","editable":false}},{"cell_type":"code","source":"#generating histograms\nplot = data.hist(figsize=(15,15))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":879},"id":"4ddBOOI-B5JL","outputId":"217187c8-181d-4552-b1a4-5479fb65ffb0","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting histograms for each numerical column in the dataframe. \nThe hist() function in pandas is used to generate histograms for all numerical columns in a dataframe.\n\n","metadata":{"id":"rKX3UMuppN2u","editable":false}},{"cell_type":"code","source":"#Dropping dependent valirable from dataframe\ndatafinal = data.drop('Nscore', axis=1)","metadata":{"id":"qDSlMy4cB8Dk","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What are the distributions of the predictor variables?**","metadata":{"id":"x2x4DgzJpX9n","editable":false}},{"cell_type":"code","source":"#checking the distribution of independent variables\nfrom statsmodels.graphics.gofplots import qqplot\ndata_norm=data[[ 'Nscore', 'Escore', 'Oscore', 'Impulsive',\n       'SS']]\nfor c in data_norm.columns[:]:\n  plt.figure(figsize=(8,5))\n  fig=qqplot(data_norm[c],line='45',fit='True')\n  plt.xticks(fontsize=13)\n  plt.yticks(fontsize=13)\n  plt.xlabel(\"Theoretical quantiles\",fontsize=15)\n  plt.ylabel(\"Sample quantiles\",fontsize=15)\n  plt.title(\"Q-Q plot of {}\".format(c),fontsize=16)\n  plt.grid(True)\n  plt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zggGQdBeztcT","outputId":"1758cf72-1c20-4065-ad60-5b9bac38cce4","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-ticks')\n\ndata.hist(bins=10, figsize=(20,20), color='#c00d00')\nplt.show();","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"oOePpTX7wBSC","outputId":"578af9c6-347e-4256-99c1-4722423af783","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the Ranges of the predictor variables and dependent variable\nplt.figure(figsize=(20,7))\nsns.boxplot(data=data)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"p96_y8XHRRV4","outputId":"b137b480-3e35-4257-8e45-8c8197a750b1","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Do the range of predictor variables make sense?**\n\n\n\n","metadata":{"id":"8clI6SOJF6Rr","editable":false}},{"cell_type":"code","source":"#Checking the Ranges of the predictor variables and dependent variable\nplt.figure(figsize=(20,7))\nsns.boxplot(data=datafinal, palette=\"Set3\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"aPrmhVYPF4L7","outputId":"27440e59-4061-4c28-e725-40bbbab2b9fd","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create box plots for all numeric variables\ndatafinal.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, figsize=(20,15))\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"id":"q2OLU-D0FwIq","outputId":"3af93fc8-3eef-46c7-84c5-7922df6e1aa1","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalizing**","metadata":{"id":"UzQLWvgfFsfl","editable":false}},{"cell_type":"code","source":"#Creating binary vaiables \n\ndata = pd.get_dummies(data)\n\n# Normalizing the data in the ids column beacuse the value is too high when compared to other independent variable\n\nfrom sklearn import preprocessing\n\n# Create x to store scaled values as floats\nx = data[['SS']].values.astype(float)\n\n# Preparing for normalizing\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n# Transform the data to fit minmax processor\nx_scaled = min_max_scaler.fit_transform(x)\n\n# Run the normalizer on the dataframe\ndata[['SS']] = pd.DataFrame(x_scaled)","metadata":{"id":"EdSx0Q9lRV1X","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data after nromalizing and feature creation\ndata.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"71gRWdeyRnDk","outputId":"373fa3a1-7c9b-469f-d654-d8b03394be6d","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the distribtion of independent(predictor) variables\ndatalist= ['Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS']\n \nfor i in datalist:\n    from scipy import stats\n    plt.figure(figsize= (5,5))\n    sns.distplot(data[i], fit = stats.norm)\n    plt.title(f\"Distribution of {i} (checking normal distribution fit)\",size = 15, weight = \"bold\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eYHdx-4HqWmv","outputId":"2d9afa92-0061-4b37-9526-f6eade3b14e8","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that Nscore, Escore,Oscore, AScore, Cscore have normal distribution whereas Impulsive and SS don't have a normal distribution.","metadata":{"id":"6AShWCqcqFL1","editable":false}},{"cell_type":"code","source":"#data after noromalizing and feature creation\ndata.describe()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"id":"QNPCC8VwFYgE","outputId":"957f403f-262d-454b-d3f9-4e817932fa8d","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the Ranges of the predictor variables and dependent variable after normalizing\nplt.figure(figsize=(20,10))\nsns.boxplot(data=data)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606},"id":"K_6qyFBTRpcK","outputId":"cb39b08f-04cc-406e-8b43-eda1d35a8057","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the correlation between all the features in the data\ndata.corr()\n","metadata":{"id":"SBT-xJP9Rwha","colab":{"base_uri":"https://localhost:8080/","height":505},"outputId":"2e53a5bb-4ad0-4e6e-876a-5249e57cb68b","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us the correlation between all the data ","metadata":{"id":"tN8_oOxiVble","editable":false}},{"cell_type":"code","source":"#the heat map of the correlation\nplt.figure(figsize=(20,7))\nsns.heatmap(df.corr(), annot=True, cmap='RdYlGn')","metadata":{"id":"-PlUjBgoR2xm","colab":{"base_uri":"https://localhost:8080/","height":452},"outputId":"39e28241-a8a2-40f5-a2d7-22216fdc6ec4","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n1.  The graph indicates that there is not much correlation between features.\n\n2.   The range lies between -0.4-1.\n\n\n\n\n\n\n\n","metadata":{"id":"OKlum_9nq1lD","editable":false}},{"cell_type":"markdown","source":" Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)","metadata":{"id":"J1U6_q_7KQpW","editable":false}},{"cell_type":"markdown","source":"**1) OLS Method**","metadata":{"id":"OGebUf4dKVPY","editable":false}},{"cell_type":"code","source":"#Using OLS for finding the p value to check the significant features\nimport statsmodels.api as sm\n\nmodel = sm.OLS(data['VSA_CL1'], data[['Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS']]).fit()\n\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"TXlZU8qAKZLh","outputId":"bb536068-660c-4832-e751-276fc608a0b6","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, Considering the significance value of 0.05\n\nThe Nscore has the p-value 0, which is lesser and hence it is a significant feature.\n\nThe Escorehas the p-value 0, which is lesser and hence it is a significant feature.\n\nThe Oscorehas the p-value 0, which is lesser and hence it is a significant feature.\n\nThe Ascore has the p-value 0, which is lesser and hence it is a significant feature.\n\nThe Impulsive has the p-value 0, which is lesser and hence it is a significant feature.","metadata":{"id":"jZpLfa5GWICY","editable":false}},{"cell_type":"markdown","source":"","metadata":{"id":"bANeQxTrsdPu","editable":false}},{"cell_type":"markdown","source":"2) Calculating Z Score Method","metadata":{"id":"TFjgycbAK9TF","editable":false}},{"cell_type":"code","source":"#Calculating Z Score\nimport pandas as pd\ndef calculate_z_scores(df):\n    return (df - df.mean()) / df.std()\ndf = pd.DataFrame(data,columns=['ID', 'Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS'])\ndf = df.apply(pd.to_numeric)\nz_scores = calculate_z_scores(df)\nprint(z_scores)","metadata":{"id":"J4lQ2Liba4XD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc7a8f68-061e-4908-ad90-be6a826a23ca","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the z score of a data point is more than 2 or less than -2 then it indicates that the data point is quite different from the other data points. Such a data point can be an outlier. ","metadata":{"id":"RzTg7xcvt7fC","editable":false}},{"cell_type":"markdown","source":"**Do the training and test sets have the same data?**","metadata":{"id":"pED6oXz3Lelp","editable":false}},{"cell_type":"markdown","source":"**Building the Model (Train ,Validation and Test split)**\n","metadata":{"id":"jLmOsB6_LmyU","editable":false}},{"cell_type":"code","source":"from sklearn.model_selection import  train_test_split\n\nX = data[['Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS']]\n\ny = data['VSA_CL1']\n\n#Spliting data into Training 76.5%, Validation set 13.5% and Test set 10%\n\nX_t, X_test, y_t, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_t, y_t, test_size=0.15, random_state=1)","metadata":{"id":"CrtgLpCbLmIi","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking the data for test, training and validation set\nX_test_plot = X_test[['Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS']]\n\n\nX_val_plot = X_val[['Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS']]\n\n\nX_train_plot = X_train[['Nscore','Escore','Oscore','AScore','Cscore','Impulsive','SS']]\n\n\n# Plotting the data to see the histogram\nfor c in X_test_plot.columns[:]:\n  plt.figure(figsize=(8,6))\n  plt.hist(X_val_plot[c], bins=20, alpha=0.5, label=\"validation\")\n  plt.hist(X_test_plot[c], bins=20, alpha=0.5, label=\"test\")\n  plt.hist(X_train_plot[c], bins=20, alpha=0.5, label=\"train\")\n  plt.xlabel(c, size=14)\n  plt.ylabel(\"Count\", size=14)\n  plt.legend(loc='upper right')\n  plt.title(\"{} distribution\".format(c))\n  plt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KTx4vEdXLeGz","outputId":"18ed8ae4-a442-4221-a6d7-e68a5324ba9c","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n*   The division is based on three factors: Validation, test and train.\n*   The division is done accurately in all the variables.\n\n","metadata":{"id":"tMexfBjauft3","editable":false}},{"cell_type":"markdown","source":"3) Gradient Boosting Classifier Results","metadata":{"id":"On13OORWM03P","editable":false}},{"cell_type":"code","source":"# Using Gradient Boosting Classifier Results\n\nfrom sklearn import ensemble\nmodel_3=ensemble.GradientBoostingClassifier()\nmodel_3.fit(X_train,y_train)\ncols=X_train.columns\nplt.figure(figsize=(15, 7))\nplt.barh(cols,model_3.feature_importances_, color =\"#00b3b3\")\nplt.title('Feature Importance ', size=14)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"46juhZ5LM5Hc","outputId":"d41eb7e0-894b-4e67-934c-9e9bfd0fffbc","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This method also gives the same results as the above two methods.","metadata":{"id":"edCy2Pz0u8IZ","editable":false}},{"cell_type":"markdown","source":"Linear Regression","metadata":{"id":"x-_hq9Z-Nukt","editable":false}},{"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import datasets, linear_model","metadata":{"id":"AEtZbuNCNtC7","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train,y_train)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U36q4jHzSyJh","outputId":"b85966d5-99f6-4538-df92-b3cc2fc5bc96","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taining a linear regression model to evaluate its performance by measuring the accracy of models.","metadata":{"id":"MUDRqUcQvO8D","editable":false}},{"cell_type":"code","source":"# Create linear regression object\nfrom sklearn.linear_model import LogisticRegression\n\nregr = LogisticRegression()\n# Train the model using the training sets\nregr.fit(X_train,y_train)\nprint('Training accuracy : ',regr.score(X_train,y_train)*100)\nprint('Test accuracy : ',regr.score(X_test,y_test)*100)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffsQeARNVc2b","outputId":"7d99268c-4392-49ae-bea9-29a15ad797e0","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model.predict(X_test)\ny_pred\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8Fx6Di0Vt2c","outputId":"48e60955-b89d-44a1-d0ca-f3f10ed99342","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predictions","metadata":{"id":"ospZUjZgVwpm","editable":false}},{"cell_type":"code","source":"# Make predictions using the training set\ny_pred = regr.predict(X_train)\n# The mean squared error\nprint('Mean squared error: %.2f'% mean_squared_error(y_train, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'% r2_score(y_train, y_pred))\nr2 = r2_score(y_train,y_pred)\nprint('R^2 score on tarining set =',r2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbgthBwkHlB7","outputId":"380ca07e-2d5f-433c-f932-16f6f59169e9","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n# The mean squared error\nprint('Mean squared error: %.2f'% mean_squared_error(y_test, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'% r2_score(y_test, y_pred))\nr2 = r2_score(y_test,y_pred)\nprint('R^2 score on test set =',r2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvEetSKRHriV","outputId":"a297f114-c7d6-4a9c-f38f-a6f667ac5937","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions using the validation set\ny_pred = regr.predict(X_val)\n\n# The mean squared error\nprint('Mean squared error: %.2f'% mean_squared_error(y_val, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f'% r2_score(y_val, y_pred))\nr2_val = r2_score(y_val,y_pred)\nprint('R^2 score on validation set =',r2_val)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_dZlxchHuu2","outputId":"efa2970e-6d38-42e4-977a-b14156227e89","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function uses the model's predictions on the X train training set to assess how well it performed.\n\nThe average squared difference between the expected and actual values is measured by the mean squared error (MSE). An improved fit of the model to the data is shown by a lower MSE.\n\nThe model's ability to account for data fluctuation is measured by the coefficient of determination (R2) An R2 of 1 denotes a perfect fit, whereas an R2 of 0 denotes that the model does not account for any of the data's variation.","metadata":{"id":"a3gzHI_NwXI-","editable":false}},{"cell_type":"markdown","source":"Important features","metadata":{"id":"U9DlVu2kbN1i","editable":false}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(regr, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","metadata":{"id":"8mOv-akhbQNO","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"975fcefd-dd51-4aec-9c34-9091251ff909","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove 1%, 5%, and 10% of your data randomly and impute the values back using at least 3 imputation methods. How well did the methods recover the missing values?**","metadata":{"id":"i1d9ZxbqH4uz","editable":false}},{"cell_type":"code","source":"percent=[0.01,0.05,0.1]\ntrain_data = data.astype(float)\ncols=list(train_data.columns)\nall_rows=[]\nall_cols=[]\nfor per in percent:\n    nan_rows_size=int(per*len(train_data))\n    random_cols=np.random.choice(np.arange(len(cols)),nan_rows_size,replace=True)\n    random_rows=np.random.choice(np.arange(len(train_data)),nan_rows_size,replace=False)\n    all_rows.append(random_rows)\n    all_cols.append(random_cols)\n    \n#creating new dataframe with 1%,5% and 10% missing values    \npredictors_missing_1_percent=train_data.copy()\nfor r_index,c_index in zip(all_rows[0],all_cols[0]):\n    predictors_missing_1_percent.at[r_index,cols[c_index]]=np.NaN\npredictors_missing_5_percent=train_data.copy()\nfor r_index,c_index in zip(all_rows[1],all_cols[1]):\n    predictors_missing_5_percent.at[r_index,cols[c_index]]=np.NaN\npredictors_missing_10_percent=train_data.copy()\nfor r_index,c_index in zip(all_rows[2],all_cols[2]):\n    predictors_missing_10_percent.at[r_index,cols[c_index]]=np.NaN","metadata":{"id":"SaPetS9sbbD-","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing missing values\nprint('Number of missing values')\nprint('1% missing',predictors_missing_1_percent.isna().sum().sum())\nprint('5% missing',predictors_missing_5_percent.isna().sum().sum())\nprint('10% missing',predictors_missing_10_percent.isna().sum().sum())\ntrain_data.head(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"bgGy9fQpbesa","outputId":"d97301ce-60b2-43a0-8736-08706e89a34d","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1%, 5%, and 10% of the rows and columns will be chosen at random, and their values will be replaced with NaN to add missing values to the original dataframe \"train data.\"\n\nSpecifying the percentages of missing values (1%, 5%, and 10%) that will be inserted into the dataframe in the \"percent\" list. After that, it constructs a list of column names and changes the dataframe's data type to float.\n\nFor the purpose of storing the randomly chosen rows and columns that will be missing values, three empty lists (all rows, all cols) are created. It multiplies the percentage by the total number of rows in the dataframe for each percentage in \"percent\" to determine how many rows (nan rows size) must have missing values.\n\nThe same number of columns and rows as the calculated number of missing data are then randomly chosen using the numpy random.choice() function. The all rows and all cols lists are then updated to include these chosen rows and columns.\n\nPredictors missing 1 percent, Predictors missing 5 percent, and Predictors missing 10 percent are the last three new dataframes it generates. A for loop is used to change the values of the rows and columns that were previously selected in each of the new dataframes with NaN after copying the original dataframe.","metadata":{"id":"cocs36CCxKC6","editable":false}},{"cell_type":"code","source":"# Dropping NaN value rows from our 1%, 5% and 10% missing values dataframe and creating new dataframes\ndf_1_percent_without_nan=predictors_missing_1_percent.dropna()\ndf_5_percent_without_nan=predictors_missing_5_percent.dropna()\ndf_10_percent_without_nan=predictors_missing_10_percent.dropna()\ndf_1_percent_without_nan.info()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6W_0V5Gsc55T","outputId":"708e958a-ddcc-4679-8b58-8b8802acda69","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the metrics library from sklearn\nfrom sklearn import metrics as sm\n# Defining the function to calculate model performance scores\ndef performnce_calcuation(y_true,y_pred):\n    acc=sm.accuracy_score(y_true,y_pred)\n    log_loss=sm.log_loss(y_true,y_pred)\n    auc=sm.roc_auc_score(y_true,y_pred)\n    confusion_matrix=sm.confusion_matrix(y_true,y_pred)\n    return acc,auc,log_loss,confusion_matrix","metadata":{"id":"I2TaG8oAbjoF","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, the data from the original dataframe is used to create new dataframes, but 1%, 5%, and 10% of the data points are replaced with NaN values. Using the dataframe's at method, values are first set to NaN before the original dataframe is replicated using the copy() method.\n\nFrom the previously constructed dataframe with missing values, a new dataframe is then created with omitted NA values. For instance, the NaN values from the predictors missing 1 percent dataframe are removed to produce df 1 percent without nan. The dataframe's information is then displayed using the info() method, including the number of non-null values in each column.","metadata":{"id":"a4Hfja9cxohd","editable":false}},{"cell_type":"markdown","source":"**Imputation** ","metadata":{"id":"CuNwZKt-brrk","editable":false}},{"cell_type":"markdown","source":"**1) Numerical values with Median and Categorical value with Mode**","metadata":{"id":"v6biToTebtwX","editable":false}},{"cell_type":"code","source":"num_cols=['Nscore', 'Escore', 'Oscore',\t'AScore',\t'Cscore',\t'Impulsive', 'SS']\n\ncat_cols=['VSA_CL1']\n\npredict_impute_1=pd.DataFrame()\npredict_impute_5=pd.DataFrame()\npredict_impute_10=pd.DataFrame()\nfor col in num_cols:\n    predict_impute_1[col]=predictors_missing_1_percent[col].fillna(predictors_missing_1_percent[col].median())\n    predict_impute_5[col]=predictors_missing_5_percent[col].fillna(predictors_missing_5_percent[col].median())\n    predict_impute_10[col]=predictors_missing_10_percent[col].fillna(predictors_missing_10_percent[col].median())\nfor col in cat_cols:\n    predict_impute_1[col]=predictors_missing_1_percent[col].fillna(predictors_missing_1_percent[col].mode())\n    predict_impute_5[col]=predictors_missing_5_percent[col].fillna(predictors_missing_1_percent[col].mode())\n    predict_impute_10[col]=predictors_missing_10_percent[col].fillna(predictors_missing_1_percent[col].mode())","metadata":{"id":"gZs75vz-bwhb","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nmodel_1=tree.DecisionTreeClassifier(random_state=42,max_depth=4)\n\npredictors=df_1_percent_without_nan.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, df_1_percent_without_nan['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_1_te,auc_1_te,log_loss_1_te,cm_1_te=performnce_calcuation(y_test,model_1.predict(X_test))\n\npredictors=df_5_percent_without_nan.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, df_5_percent_without_nan['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_2_te,auc_2_te,log_loss_2_te,cm_2_te=performnce_calcuation(y_test,model_1.predict(X_test))\n\npredictors=df_10_percent_without_nan.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, df_10_percent_without_nan['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_3_te,auc_3_te,log_loss_3_te,cm_3_te=performnce_calcuation(y_test,model_1.predict(X_test))","metadata":{"id":"vAw1VOlobrU5","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-------------------------------\")\nprint(f'For 1% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_1_te:.2f}')\nprint(f'AUC: {auc_1_te:.2f}')\nprint(f'Log Loss: {log_loss_1_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_1_te}')\nprint(\"\\n-------------------------------\")\nprint(f'For 5% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_2_te:.2f}')\nprint(f'AUC: {auc_2_te:.2f}')\nprint(f'Log Loss: {log_loss_2_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_2_te}')\nprint(\"\\n-------------------------------\")\nprint(f'For 10% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_3_te:.2f}')\nprint(f'AUC: {auc_3_te:.2f}')\nprint(f'Log Loss: {log_loss_3_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_3_te}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWUiO3p2ID93","outputId":"81cd446b-a640-48fe-d6cb-45a33f86ee9a","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_firssthand_accuracy = 0.96\nplt.figure(figsize=(8, 6))\naccuracy=[model_firssthand_accuracy,accuracy_1_te,accuracy_2_te,accuracy_3_te]\nlog_loss=[log_loss_1_te,log_loss_2_te,log_loss_3_te]\nlabel=['No data imputation','1% imputation','5% imputation','10% imputation']\nplt.title('Accuracy plot vs % of data imputation')\nplt.ylabel('Accuracy')\nplt.xlabel('% of data imputed')\nplt.plot(label,accuracy)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"fgrjbpRqIHhR","outputId":"a038d9e0-953b-407b-e44b-df04bb36c8d4","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There was a steady increase of accuracy for 1% imputation, later it decreased for 5% but then again it increased for 10% imputation.","metadata":{"id":"gLc8azShx-Q0","editable":false}},{"cell_type":"markdown","source":"Using KNN imputer","metadata":{"id":"jBgMDYq4ILad","editable":false}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\n\npredictors=predict_impute_1.drop('VSA_CL1',axis=1)\nall_col=list(predictors.columns)\nfinal=imputer.fit_transform(predictors)\nimputed_df_1=pd.DataFrame(final,columns=all_col)\n\npredictors=predict_impute_5.drop('VSA_CL1',axis=1)\nall_col=list(predictors.columns)\nfinal=imputer.fit_transform(predictors)\nimputed_df_5=pd.DataFrame(final,columns=all_col)\n\npredictors=predict_impute_10.drop('VSA_CL1',axis=1)\nall_col=list(predictors.columns)\nfinal=imputer.fit_transform(predictors)\nimputed_df_10=pd.DataFrame(final,columns=all_col)","metadata":{"id":"UcQ5ooDnIPm7","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1=tree.DecisionTreeClassifier(random_state=42,max_depth=2)\npredict_impute_1 = predict_impute_1.dropna()\npredict_impute_5 = predict_impute_5.dropna()\npredict_impute_10 = predict_impute_10.dropna()\n\npredictors=predict_impute_1.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, predict_impute_1['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_1_te,auc_1_te,log_loss_1_te,cm_1_te=performnce_calcuation(y_test,model_1.predict(X_test))\n\npredictors=predict_impute_5.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, predict_impute_5['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_2_te,auc_2_te,log_loss_2_te,cm_2_te=performnce_calcuation(y_test,model_1.predict(X_test))\n\npredictors=predict_impute_10.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, predict_impute_10['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_3_te,auc_3_te,log_loss_3_te,cm_3_te=performnce_calcuation(y_test,model_1.predict(X_test))\n","metadata":{"id":"hRRw8kiQIZy4","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-------------------------------\")\nprint(f'For 1% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_1_te:.2f}')\nprint(f'AUC: {auc_1_te:.2f}')\nprint(f'Log Loss: {log_loss_1_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_1_te}')\nprint(\"\\n-------------------------------\")\nprint(f'For 5% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_2_te:.2f}')\nprint(f'AUC: {auc_2_te:.2f}')\nprint(f'Log Loss: {log_loss_2_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_2_te}')\nprint(\"\\n-------------------------------\")\nprint(f'For 10% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_3_te:.2f}')\nprint(f'AUC: {auc_3_te:.2f}')\nprint(f'Log Loss: {log_loss_3_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_3_te}')\nprint(\"\\n-------------------------------\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMRrMuByIsiO","outputId":"67022b1e-07f7-4c30-b30b-f981a560f9de","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_firssthand_accuracy = 0.97\nplt.figure(figsize=(8, 6))\naccuracy=[model_firssthand_accuracy,accuracy_1_te,accuracy_2_te,accuracy_3_te]\nlog_loss=[log_loss_1_te,log_loss_2_te,log_loss_3_te]\nlabel=['No data imputation','1% imputation','5% imputation','10% imputation']\nplt.title('Accuracy plot vs % of data imputation')\nplt.ylabel('Accuracy')\nplt.xlabel('% of data imputed')\nplt.plot(label,accuracy)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"-Ktpsl6WIwfj","outputId":"9eee3dc6-9027-4ca3-e92f-a1f14366bbc7","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MICE Imputation technique","metadata":{"id":"fROJELlII1Ty","editable":false}},{"cell_type":"code","source":"!pip install fancyimpute\n#from sklearn.impute import IterativeImputer\nfrom fancyimpute import IterativeImputer as MICE\nimport pandas as pd\n\n\n# create a copy of the original dataframe with missing values\ndf_missing = predictors\ndf_missing.iloc[2:5,3:6] = np.nan\n\n# create an imputer object\nimputer = MICE()\n\n# fit the imputer on the data\nimputer.fit(predictors)\n\n#MICE().fit_transform(df)\n\n\n# perform the imputation\ndf_imputed = imputer.transform(predictors)\n\npredictors=predict_impute_1.drop('VSA_CL1',axis=1)\nall_col=list(predictors.columns)\nfinal=imputer.fit_transform(predictors)\nimputed_df_1=pd.DataFrame(final,columns=all_col)\n\npredictors=predict_impute_5.drop('VSA_CL1',axis=1)\nall_col=list(predictors.columns)\nfinal=imputer.fit_transform(predictors)\nimputed_df_5=pd.DataFrame(final,columns=all_col)\n\npredictors=predict_impute_10.drop('VSA_CL1',axis=1)\nall_col=list(predictors.columns)\nfinal=imputer.fit_transform(predictors)\nimputed_df_10=pd.DataFrame(final,columns=all_col)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NbCZ3gJzI2eB","outputId":"27b334e3-097c-47ec-e9a8-53d057a3737d","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1=tree.DecisionTreeClassifier(random_state=42,max_depth=4)\n\npredictors=df_1_percent_without_nan.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, df_1_percent_without_nan['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_1_te,auc_1_te,log_loss_1_te,cm_1_te=performnce_calcuation(y_test,model_1.predict(X_test))\n\npredictors=df_5_percent_without_nan.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, df_5_percent_without_nan['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_2_te,auc_2_te,log_loss_2_te,cm_2_te=performnce_calcuation(y_test,model_1.predict(X_test))\n\npredictors=df_10_percent_without_nan.drop('VSA_CL1',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(predictors, df_10_percent_without_nan['VSA_CL1'], test_size=0.25, random_state=42)\nmodel_1.fit(X_train,y_train)\n\naccuracy_3_te,auc_3_te,log_loss_3_te,cm_3_te=performnce_calcuation(y_test,model_1.predict(X_test))\n","metadata":{"id":"jh-o8RSUI_gT","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-------------------------------\")\nprint(f'For 1% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_1_te:.2f}')\nprint(f'AUC: {auc_1_te:.2f}')\nprint(f'Log Loss: {log_loss_1_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_1_te}')\nprint(\"\\n-------------------------------\")\nprint(f'For 5% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_2_te:.2f}')\nprint(f'AUC: {auc_2_te:.2f}')\nprint(f'Log Loss: {log_loss_2_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_2_te}')\nprint(\"\\n-------------------------------\")\nprint(f'For 10% Imputation Dataset')\nprint(\"-------------------------------\")\nprint(f'Accuracy: {accuracy_3_te:.2f}')\nprint(f'AUC: {auc_3_te:.2f}')\nprint(f'Log Loss: {log_loss_3_te:.2f}')\nprint(f'Confusion Matrix:\\n {cm_3_te}')\nprint(\"\\n-------------------------------\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5X0V8ijqJM07","outputId":"c13cc18d-fa3a-4081-b0ba-84d1c19d29ad","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_firssthand_accuracy = 0.97\nplt.figure(figsize=(8, 6))\naccuracy=[model_firssthand_accuracy,accuracy_1_te,accuracy_2_te,accuracy_3_te]\nlog_loss=[log_loss_1_te,log_loss_2_te,log_loss_3_te]\nlabel=['No data imputation','1% imputation','5% imputation','10% imputation']\nplt.title('Accuracy plot vs % of data imputation')\nplt.ylabel('Accuracy')\nplt.xlabel('% of data imputed')\nplt.plot(label,accuracy)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"n8z7-LFtJQ6a","outputId":"37996cfb-d91c-4256-d49b-054a74c01f0f","editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph shows the accueacy is at 0 when there is no data imputed but when it goes to 1% increases, there is a steady increase and at 5%, it notices a gradual decrease and then goes up again.","metadata":{"id":"tEyLyR40UOjM","editable":false}},{"cell_type":"markdown","source":"**CONCLUSION**","metadata":{"id":"uEtNF_xGTyCr","editable":false}},{"cell_type":"markdown","source":"The analysis of the data set performed above makes it clear that no colinearity exists and that the importance of the dependent variables  and all others is significant. Also recognized the significance of the factors for data training.","metadata":{"id":"k2hyYZIcT3Ny","editable":false}},{"cell_type":"markdown","source":"**REFERENCES**\n","metadata":{"id":"PFW5um6ySs61","editable":false}},{"cell_type":"markdown","source":"*  https://github.com/aiskunks/Skunks_Skool\n*  https://seaborn.pydata.org\n*   https://towardsdatascience.com/min-max-scaler-in-machine-learning-fad2f2e1daf7\n*    https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n*    https://stackoverflow.com/questions/54059964/can-not-use-mice-from-fancyimputer-python\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{"id":"Wfi1KsCpS4Tu","editable":false}},{"cell_type":"markdown","source":"**LICENSING**","metadata":{"id":"1msy96Gvzggz","editable":false}},{"cell_type":"markdown","source":"2023 CopyrightÂ Arpita Bhagat\n\nThe right to deal in the software without restriction, including without limitation the right to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software, is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), subject to the following requirements:\n\nAll copies or substantial parts of the Software must carry the aforementioned copyright notice and this permission notice.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","metadata":{"id":"Im9CjCiSzMAg","editable":false}},{"cell_type":"code","source":"","metadata":{"id":"IJrdw7ngJR_M","editable":false},"execution_count":null,"outputs":[]}]}